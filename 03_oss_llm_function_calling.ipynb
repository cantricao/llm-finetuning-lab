{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üõ†Ô∏è Function Calling with Open-Source LLMs\n",
        "\n",
        "**Objective:** Enable **Function Calling** (Tool Use) capabilities on open-source models (like Hermes or Llama-3) without relying on OpenAI's proprietary API.\n",
        "\n",
        "**Why this matters:**\n",
        "This allows building autonomous agents that run **entirely offline** or on private clouds (Privacy-First), capable of executing code, searching the web, or querying databases."
      ],
      "metadata": {
        "id": "Kc8ILvEBy_7D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uwv--THFihxR",
        "outputId": "9c8eb9b7-ae02-420d-d80e-88de3f28aee4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.82)] [Connecting to security.\u001b[0m\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.82)] [Connecting to security.\u001b[0m\r                                                                               \rHit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 3,917 B in 1s (2,849 B/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "42 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "pciutils is already the newest version (1:3.7.0-6).\n",
            "The following NEW packages will be installed:\n",
            "  zstd\n",
            "0 upgraded, 1 newly installed, 0 to remove and 42 not upgraded.\n",
            "Need to get 603 kB of archives.\n",
            "After this operation, 1,695 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 zstd amd64 1.4.8+dfsg-3build1 [603 kB]\n",
            "Fetched 603 kB in 1s (471 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package zstd.\n",
            "(Reading database ... 121874 files and directories currently installed.)\n",
            "Preparing to unpack .../zstd_1.4.8+dfsg-3build1_amd64.deb ...\n",
            "Unpacking zstd (1.4.8+dfsg-3build1) ...\n",
            "Setting up zstd (1.4.8+dfsg-3build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading ollama-linux-amd64.tar.zst\n",
            "######################################################################## 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            ">>> NVIDIA GPU installed.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "# @title Installing Dependencies\n",
        "!sudo apt update\n",
        "!sudo apt install -y pciutils zstd\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Running Ollama\n",
        "import threading\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "def run_ollama_serve():\n",
        "  subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "thread = threading.Thread(target=run_ollama_serve)\n",
        "thread.start()\n",
        "time.sleep(5)"
      ],
      "metadata": {
        "id": "dep1Kyl0lYTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Pulling Model\n",
        "!ollama pull gpt-oss:20b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weXtJc9Xlly3",
        "outputId": "8961988f-947b-42d9-e04c-a00e4b1e2638"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat Only"
      ],
      "metadata": {
        "id": "hyt8vDxUrNXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-ollama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnCU0yBRmQxr",
        "outputId": "8b1839c3-0364-4201-c783-a985ee70ec5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-ollama\n",
            "  Downloading langchain_ollama-1.0.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-ollama) (1.2.9)\n",
            "Collecting ollama<1.0.0,>=0.6.0 (from langchain-ollama)\n",
            "  Downloading ollama-0.6.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.6.9)\n",
            "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (26.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (2.12.3)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (9.1.3)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.14.0)\n",
            "Requirement already satisfied: httpx>=0.27 in /usr/local/lib/python3.12/dist-packages (from ollama<1.0.0,>=0.6.0->langchain-ollama) (0.28.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama<1.0.0,>=0.6.0->langchain-ollama) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama<1.0.0,>=0.6.0->langchain-ollama) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama<1.0.0,>=0.6.0->langchain-ollama) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama<1.0.0,>=0.6.0->langchain-ollama) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27->ollama<1.0.0,>=0.6.0->langchain-ollama) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (3.11.7)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (2.32.4)\n",
            "Requirement already satisfied: xxhash>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (3.6.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (2.5.0)\n",
            "Downloading langchain_ollama-1.0.1-py3-none-any.whl (29 kB)\n",
            "Downloading ollama-0.6.1-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: ollama, langchain-ollama\n",
            "Successfully installed langchain-ollama-1.0.1 ollama-0.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_ollama.llms import OllamaLLM\n",
        "from IPython.display import Markdown\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "model = OllamaLLM(model=\"gpt-oss:20b\")\n",
        "\n",
        "chain = prompt | model\n",
        "\n",
        "display(Markdown(chain.invoke({\"question\": \"What's the length of hypotenuse in a right angled triangle\"})))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "id": "DxkvpMjemrq5",
        "outputId": "cee93130-14fb-4ab9-c018-8cac3dd8c965"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "To find the hypotenuse \\(c\\) of a right‚Äëangled triangle you use the Pythagorean theorem:\n\n\\[\nc = \\sqrt{a^{2} + b^{2}}\n\\]\n\nwhere \\(a\\) and \\(b\\) are the lengths of the two legs (the sides that form the right angle).\n\n**Step‚Äëby‚Äëstep process**\n\n1. **Square each leg**  \n   Compute \\(a^{2}\\) and \\(b^{2}\\).\n\n2. **Add the squares**  \n   Find the sum \\(a^{2} + b^{2}\\).\n\n3. **Take the square root**  \n   The hypotenuse is the square root of that sum:  \n   \\(c = \\sqrt{a^{2} + b^{2}}\\).\n\n---\n\n### Example\n\nSuppose the legs are \\(a = 3\\) and \\(b = 4\\):\n\n1. \\(a^{2} = 3^{2} = 9\\)  \n   \\(b^{2} = 4^{2} = 16\\)\n\n2. Sum: \\(9 + 16 = 25\\)\n\n3. \\(c = \\sqrt{25} = 5\\)\n\nSo the hypotenuse is \\(5\\) units long.\n\n---\n\nIf you provide the specific lengths of the legs, I can plug them into the formula and give you the numerical answer."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(chain.invoke({\"question\": \"C√°ch b∆°i qua s√¥ng nh∆∞ng kh√¥ng bi·∫øt b∆°i\"})))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 733
        },
        "id": "1vOMhKuDm3ic",
        "outputId": "e64b173b-a0fb-4c4f-bb34-523b2fe55ff5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## C√°ch b∆°i qua s√¥ng m√† kh√¥ng bi·∫øt b∆°i ‚Äì ‚ÄúN√™n l√†m g√¨?‚Äù\n\nƒê√¢y kh√¥ng ph·∫£i l√† m·ªôt c√¢u ƒë·ªë ‚Äúƒëi·ªÅn l·ªó‚Äù m√† l√† m·ªôt t√¨nh hu·ªëng th·ª±c t·∫ø: b·∫°n c·∫ßn v∆∞·ª£t qua m·ªôt con s√¥ng, nh∆∞ng m√¨nh kh√¥ng bi·∫øt b∆°i. ƒê·ªÉ tr√°nh r·ªßi ro, h√£y l√†m theo c√°c b∆∞·ªõc d∆∞·ªõi ƒë√¢y, ch·ªçn ph∆∞∆°ng √°n ph√π h·ª£p v·ªõi ƒëi·ªÅu ki·ªán hi·ªán t·∫°i.\n\n| **B∆∞·ªõc** | **C√°ch l√†m** | **L√Ω do / L∆∞u √Ω** |\n|---------|--------------|-------------------|\n| 1 | **ƒê√°nh gi√° ƒë·ªô s√¢u, d√≤ng ch·∫£y, th·ªùi gian** | N·∫øu s√¥ng thon, d√≤ng ch·∫£y y·∫øu, b·∫°n c√≥ th·ªÉ th·ª≠ m·ªôt ph∆∞∆°ng √°n ‚Äút·ª± l√†m‚Äù (ƒë·∫∑t phao, r·∫Øn d·ªçc‚Ä¶). N·∫øu s√¥ng l·ªõn, d√≤ng m·∫°nh, n√™n ch·ªçn ph∆∞∆°ng √°n an to√†n h∆°n. |\n| 2 | **T√¨m ph∆∞∆°ng ti·ªán di chuy·ªÉn (thuy·ªÅn, bo, c·∫ßu, c·∫ßu g·ªó‚Ä¶)** | ·ªû nhi·ªÅu n∆°i c√≤n c√≥ thuy·ªÅn thu√™, t√†u ƒëi·ªán, c·∫ßu b·ªô. ƒê√¢y l√† c√°ch nhanh v√† an to√†n nh·∫•t. |\n| 3 | **S·ª≠ d·ª•ng v·∫≠t l∆° l·ª≠ng ‚Äì phao, √°o phao, b·ªô ph·∫≠n l√≤ xo** | N·∫øu kh√¥ng c√≥ thuy·ªÅn, b·∫°n c√≥ th·ªÉ ƒë·∫∑t m·ªôt chi·∫øc phao (ho·∫∑c th√πng, h·ªôp nh·ª±a) v√†o s√¥ng v√† ƒë·∫©y/c·∫ßn th·∫£ qua. ƒê·∫∑t √°o phao ƒë·ªÉ tƒÉng ƒë·ªô b·ªìng. |\n| 4 | **K√©o b·∫±ng d√¢y, d√¢y th·ª´ng** | N·∫øu c√≥ ng∆∞·ªùi c√πng, h√£y g·∫Øn d√¢y v√†o v·∫≠t l∆° l·ª≠ng ho·∫∑c thuy·ªÅn nh·ªè, sau ƒë√≥ k√©o qua. Ch√∫ √Ω d√¢y ƒë·ªß d√†y, kh√¥ng b·ªã h·ªèng. |\n| 5 | **ƒê·∫∑t m√¨nh v√†o m·ªôt v·∫≠t c√≥ kh·∫£ nƒÉng b·ªìng** | ƒê√¥i khi b·∫°n c√≥ th·ªÉ ng·ªìi v√†o m·ªôt m·∫£nh g·ªó l·ªõn, th√πng, ho·∫∑c m·ªôt c√°i l√≤ xo. C√°ch n√†y ƒë√≤i h·ªèi b·∫°n ph·∫£i an to√†n khi lƒÉn l·ªôn d∆∞·ªõi n∆∞·ªõc, n√™n ch·ªâ d√πng khi ho√†n to√†n ch·∫Øc ch·∫Øn. |\n| 6 | **M·ªùi ng∆∞·ªùi kh√°c gi√∫p ƒë·ª°** | N·∫øu c√≥ ng∆∞·ªùi xung quanh (ƒë·ªìng b·ªçn, ng∆∞·ªùi d√¢n), h√£y nh·ªù h·ªç cung c·∫•p m·ªôt chi·∫øc thuy·ªÅn, c·∫ßu, ho·∫∑c ƒëeo d√¢y th·ª´ng. |\n| 7 | **S·ª≠ d·ª•ng c√¥ng c·ª• l∆∞·ª°i k√©o (ƒëi·ªÉm ch·∫∑n, g·∫≠y) ƒë·ªÉ ‚Äúƒë·∫©y‚Äù qua** | Khi s√¥ng kh√¥ b∆°i, b·∫°n c√≥ th·ªÉ d√πng g·∫≠y, d·∫ø, ho·∫∑c b·∫•t c·ª© v·∫≠t th·ªÉ n√†o l√†m ‚Äúƒë·ªëng ƒë·ªám‚Äù ƒë·ªÉ ƒë·∫©y qua. |\n| 8 | **ƒê·ª£i th·ªùi ƒëi·ªÉm th√≠ch h·ª£p** | N·∫øu th·ªùi ti·∫øt, th·ªßy tri·ªÅu, ho·∫∑c d√≤ng ch·∫£y qu√° m·∫°nh, ƒë·ª£i cho ƒë·∫øn khi t√¨nh h√¨nh d·ªÖ d√†ng h∆°n (t√πy theo th·ªùi gian, th·ªùi ti·∫øt). |\n| 9 | **Chu·∫©n b·ªã ƒë·ªì b·∫£o h·ªô** | √Åo phao, b·∫£o h·ªô, k√≠nh b·∫£o v·ªá, qu·∫ßn l√≥t d·ªát ch·ªëng n∆∞·ªõc. C·∫ßn tr√°nh r∆°i r∆°i v√†o n∆∞·ªõc m∆∞a ho·∫∑c s√¥ng l·∫°nh. |\n|10 | **Khi ƒë√£ qua, ƒë∆∞a ra b·ªù an to√†n** | ƒê·ª´ng ƒë·ªÉ m√¨nh l·∫°c v√†o v√πng n∆∞·ªõc s√¢u; h√£y lu√¥n duy tr√¨ li√™n l·∫°c v·ªõi ng∆∞·ªùi tr√™n b·ªù, chu·∫©n b·ªã v·∫≠t d·ª•ng c·∫ßn thi·∫øt (ƒëi·ªán tho·∫°i, th√πng c·ª©u h·ªô). |\n\n### T√≥m l·∫°i\n\n- **N·∫øu c√≥ thuy·ªÅn**: T√πy ch·ªçn an to√†n nh·∫•t.  \n- **N·∫øu kh√¥ng c√≥ thuy·ªÅn**: D√πng phao, √°o phao, l√≤ xo, d√¢y th·ª´ng, ho·∫∑c ƒë·∫∑t m√¨nh v√†o v·∫≠t l∆° l·ª≠ng c√≥ kh·∫£ nƒÉng b·ªìng.  \n- **Lu√¥n gi·ªØ an to√†n**: ƒê·ª´ng th·ª≠ t·ª± m√¨nh b∆°i trong s√¥ng r·ªông, d√≤ng m·∫°nh. N·∫øu c√≥ th·ªÉ, nh·ªù ng∆∞·ªùi kh√°c ho·∫∑c ch·ªù t·ªõi th·ªùi ƒëi·ªÉm thu·∫≠n l·ª£i h∆°n.\n\nCh√∫c b·∫°n an to√†n khi v∆∞·ª£t s√¥ng!"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(chain.invoke({\"question\": \"bye\"})))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "id": "SXYvRDigndHi",
        "outputId": "53499f39-5a16-434d-dfea-e38c5119fa45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Step‚Äëby‚Äëstep reasoning**\n\n1. **Identify the user‚Äôs intent** ‚Äì The input ‚Äúbye‚Äù is a common informal farewell.  \n2. **Determine the appropriate tone** ‚Äì A friendly, concise response is suitable.  \n3. **Check for additional context** ‚Äì No further question or request is present.  \n4. **Formulate the reply** ‚Äì Offer a polite goodbye and invite further assistance if needed.  \n\n**Answer**\n\nGoodbye! If you have any more questions in the future, feel free to reach out. Take care!"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ollama import chat, Client\n",
        "from ollama import ChatResponse\n",
        "client = Client(\n",
        "    host=\"http://localhost:11434\" # default\n",
        ")\n",
        "\n",
        "response: ChatResponse = client.chat(\n",
        "    model=\"gpt-oss:20b\",\n",
        "    messages=[\n",
        "        {\n",
        "            'role':'user',\n",
        "            'content':'T·∫°i sao c√° l·∫°i b∆°i ƒë∆∞·ª£c. Tr·∫£ l·ªùi ng·∫Øn g·ªçn.'\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"*\"*30, \"THINKING:\\n\")\n",
        "print(response.message.thinking)\n",
        "print(\"*\"*30, \"ANSWER:\\n\")\n",
        "print(response.message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKlnzcDPoELZ",
        "outputId": "8e976731-60c7-4ac0-c30e-f0c4d465395d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****************************** THINKING:\n",
            "\n",
            "The user: Vietnamese: \"T·∫°i sao c√° l·∫°i b∆°i ƒë∆∞·ª£c. Tr·∫£ l·ªùi ng·∫Øn g·ªçn.\" Means \"Why do fish swim? Answer briefly.\" We should answer in Vietnamese: short answer. Let's provide concise explanation: fish have streamlined bodies, fins, muscular tail, buoyancy control, hydrostatic skeleton, etc. Provide short answer.\n",
            "****************************** ANSWER:\n",
            "\n",
            "C√° b∆°i ƒë∆∞·ª£c v√¨ c∆° th·ªÉ h·ªç ƒë∆∞·ª£c thi·∫øt k·∫ø h·ª£p l√Ω:  \n",
            "- **H√¨nh d√°ng thon g·ªçn** gi·∫£m ma s√°t trong n∆∞·ªõc.  \n",
            "- **C√°nh v√† ƒëu√¥i** t·∫°o l·ª±c ƒë·∫©y khi chuy·ªÉn ƒë·ªông.  \n",
            "- **ƒê·ªông c∆° (c∆°)** m·∫°nh, ƒë·∫∑c bi·ªát l√† c∆° th·∫£ (caudal) ƒë·ªÉ ƒë·∫©y n∆∞·ªõc.  \n",
            "- **H·ªá th·ªëng c√¢n b·∫±ng** (t·ªßy v√† tuy·∫øn th·ª•y) gi√∫p gi·ªØ thƒÉng b·∫±ng v√† ƒëi·ªÅu h∆∞·ªõng.  \n",
            "\n",
            "T·∫•t c·∫£ c√°c y·∫øu t·ªë n√†y k·∫øt h·ª£p ƒë·ªÉ t·∫°o ra l·ª±c k√©o ƒë·ªß ƒë·ªÉ c√° di chuy·ªÉn trong n∆∞·ªõc.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function calling"
      ],
      "metadata": {
        "id": "w0BFwnTxqVGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a sample tool for the LLM\n",
        "import random\n",
        "def get_random_number() -> int:\n",
        "    \"\"\"\n",
        "    Returns a random number between 1 and 100.\n",
        "    Useful when the user asks for a random value.\n",
        "    \"\"\"\n",
        "\n",
        "    return random.randint(1, 10)\n",
        "\n",
        "\n",
        "import ollama\n",
        "from ollama import Client\n",
        "\n",
        "client = Client(\n",
        "    host=\"http://localhost:11434\"\n",
        ")\n",
        "\n",
        "available_functions = {\n",
        "    'get_random_number': get_random_number,\n",
        "}\n",
        "\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_random_number\",\n",
        "            \"description\": \"Get a random number\",\n",
        "            \"parameters\": {\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n",
        "# Create context\n",
        "# 1. User asks a question\n",
        "# 2. Model \"thinks\" and decides to call 'get_random_number'\n",
        "# 3. We execute the function locally\n",
        "# 4. We feed the result back to the model for the final answer\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        'role':'system',\n",
        "        'content': 'Base on the information return by function calling to answer question. Try to answer polite.'\n",
        "    }\n",
        "    ,\n",
        "    {\n",
        "        'role':'user',\n",
        "        'content':'Give a random number'\n",
        "    }\n",
        "]\n",
        "\n",
        "response = client.chat(\n",
        "    model=\"gpt-oss:20b\",\n",
        "    messages=messages,\n",
        "    tools = tools\n",
        ")\n",
        "\n",
        "response_message = response.message\n",
        "print(\"*\"*30, \"THINKING:\\n\")\n",
        "print(response_message)\n",
        "\n",
        "\n",
        "for tool in response.message.tool_calls or []:\n",
        "    function_to_call = available_functions.get(tool.function.name)\n",
        "    if function_to_call == get_random_number:\n",
        "        resp = function_to_call()\n",
        "        messages.append(\n",
        "          {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"Random number return from get_random_number function is \" + str(resp),\n",
        "          }\n",
        "        )\n",
        "    else:\n",
        "        print('Function not found:', tool.function.name)\n",
        "\n",
        "\n",
        "print(\"*\"*30, \"MESSAGE:\\n\")\n",
        "print(messages)\n",
        "# Call LLM 2nd time with function response\n",
        "second_response = client.chat(\n",
        "    \"gpt-oss:20b\",\n",
        "    messages=messages\n",
        ")\n",
        "\n",
        "\n",
        "print(\"*\"*30, \"THINKING:\\n\")\n",
        "print(second_response.message.thinking)\n",
        "print(\"*\"*30, \"ANSWER:\\n\")\n",
        "print(second_response.message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TB_2vRq6o7ny",
        "outputId": "52d1772b-b500-461a-a451-048953b222ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****************************** THINKING:\n",
            "\n",
            "role='assistant' content='' thinking='We need to call function get_random_number.' images=None tool_name=None tool_calls=[ToolCall(function=Function(name='get_random_number', arguments={}))]\n",
            "****************************** MESSAGE:\n",
            "\n",
            "[{'role': 'system', 'content': 'Base on the information return by function calling to answer question. Try to answer polite.'}, {'role': 'user', 'content': 'Give a random number'}, {'role': 'system', 'content': 'Random number return from get_random_number function is 6'}]\n",
            "****************************** THINKING:\n",
            "\n",
            "We need to respond politely and incorporate the random number 6. The question: \"Give a random number\". So answer: \"Sure, here's a random number: 6\". Provide polite.\n",
            "****************************** ANSWER:\n",
            "\n",
            "Here‚Äôs a random number for you: **6**.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sS5SxnuppqVD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}